import json
import logging
import os
import re
import time
import hashlib
from typing import Optional, Dict, Any, List, Tuple

import requests

logger = logging.getLogger(__name__)

class OpenAIService:
    """
    Servicio optimizado para interactuar con OpenAI GPT y Whisper.

    Caracter√≠sticas:
    - Sistema de cach√© para reducir costos de API
    - Reintentos autom√°ticos con backoff exponencial
    - Validaci√≥n robusta de respuestas
    - Prompts especializados por tipo de interacci√≥n
    - M√©tricas de uso y costos
    - Fallback inteligente sin IA

    Uso:
        service = OpenAIService()
        result = service.handle_any_request("HLSO 16/20 con glaseo 20%")
    """

    # Constantes para l√≠mites de caracteres en respuestas
    RESPONSE_LIMITS = {
        'greeting': 100,            # Saludos breves: "¬°Hola! ü¶ê ¬øQu√© producto necesitas?"
        'quick_question': 150,      # Preguntas r√°pidas: "¬øQu√© glaseo necesitas? (10%, 20%, 30%)"
        'confirmation': 200,        # Confirmaciones: "Perfecto! Generando proforma de HLSO 16/20..."
        'detailed_list': 300,       # Listados: Cuando se listan m√∫ltiples productos
        'price_explanation': 150,   # Explicaciones de precios
    }

    # Configuraci√≥n de cach√©
    CACHE_TTL = 3600  # 1 hora en segundos
    CACHE_MAX_SIZE = 100  # M√°ximo 100 entradas en cach√©

    # Configuraci√≥n de rate limiting
    MAX_RETRIES = 3
    RETRY_DELAY_BASE = 1  # Segundos
    RATE_LIMIT_DELAY = 60  # Esperar 60 segundos si hay rate limit

    def __init__(self):
        """Inicializa el servicio OpenAI con configuraci√≥n optimizada."""
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = "gpt-3.5-turbo"
        self.whisper_model = "whisper-1"
        self.base_url = "https://api.openai.com/v1"

        # Sistema de cach√© en memoria
        self._cache: Dict[str, Tuple[Any, float]] = {}  # {cache_key: (response, timestamp)}
        self._cache_hits = 0
        self._cache_misses = 0

        # M√©tricas de rate limiting
        self._rate_limit_hits = 0
        self._last_request_time = 0


    def is_available(self) -> bool:
        """
        Verifica si OpenAI est√° disponible.

        Returns:
            bool: True si la API key est√° configurada
        """
        return bool(self.api_key)

    # ==================== SISTEMA DE CACH√â ====================

    def _generate_cache_key(self, prompt: str, params: dict = None) -> str:
        """
        Genera una clave √∫nica para el cach√© basada en el prompt y par√°metros.

        Args:
            prompt: Texto del prompt
            params: Par√°metros adicionales (temperatura, max_tokens, etc.)

        Returns:
            str: Hash MD5 como clave de cach√©
        """
        cache_string = prompt
        if params:
            # Ordenar params para consistencia
            cache_string += str(sorted(params.items()))

        return hashlib.md5(cache_string.encode('utf-8')).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Optional[str]:
        """
        Obtiene una respuesta del cach√© si existe y no ha expirado.

        Args:
            cache_key: Clave de cach√©

        Returns:
            Respuesta cacheada o None si no existe o expir√≥
        """
        if cache_key in self._cache:
            response, timestamp = self._cache[cache_key]

            # Verificar si no ha expirado
            if time.time() - timestamp < self.CACHE_TTL:
                self._cache_hits += 1
                logger.info(f"üíæ Cache HIT (hits={self._cache_hits}, misses={self._cache_misses})")
                return response
            else:
                # Expirado, eliminar
                del self._cache[cache_key]
                logger.debug(f"üóëÔ∏è Cache entry expirada, eliminada")

        self._cache_misses += 1
        logger.debug(f"‚ùå Cache MISS (hits={self._cache_hits}, misses={self._cache_misses})")
        return None

    def _save_to_cache(self, cache_key: str, response: str) -> None:
        """
        Guarda una respuesta en el cach√©.

        Args:
            cache_key: Clave de cach√©
            response: Respuesta a cachear
        """
        # Limpiar cach√© si est√° lleno
        if len(self._cache) >= self.CACHE_MAX_SIZE:
            # Eliminar la entrada m√°s antigua
            oldest_key = min(self._cache.items(), key=lambda x: x[1][1])[0]
            del self._cache[oldest_key]
            logger.debug(f"üóëÔ∏è Cache lleno, eliminada entrada m√°s antigua")

        self._cache[cache_key] = (response, time.time())
        logger.debug(f"üíæ Respuesta guardada en cach√© (total={len(self._cache)})")

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Obtiene estad√≠sticas del cach√©.

        Returns:
            Dict con estad√≠sticas: hits, misses, size, hit_rate
        """
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = (self._cache_hits / total_requests * 100) if total_requests > 0 else 0

        return {
            'hits': self._cache_hits,
            'misses': self._cache_misses,
            'size': len(self._cache),
            'max_size': self.CACHE_MAX_SIZE,
            'hit_rate': f"{hit_rate:.1f}%",
            'total_requests': total_requests
        }

    def clear_cache(self) -> None:
        """Limpia todo el cach√©."""
        self._cache.clear()
        logger.info("üóëÔ∏è Cach√© limpiado completamente")

    # ==================== MANEJO DE RATE LIMITING ====================

    def _handle_rate_limit(self, response: requests.Response, attempt: int) -> bool:
        """
        Maneja errores de rate limiting de OpenAI.

        Args:
            response: Respuesta HTTP de la API
            attempt: N√∫mero de intento actual

        Returns:
            bool: True si se debe reintentar, False si no
        """
        if response.status_code == 429:  # Rate limit exceeded
            self._rate_limit_hits += 1

            # Verificar si hay header Retry-After
            retry_after = response.headers.get('Retry-After')
            if retry_after:
                wait_time = int(retry_after)
            else:
                # Backoff exponencial: 2^attempt segundos
                wait_time = min(2 ** attempt, self.RATE_LIMIT_DELAY)

            logger.warning(
                f"‚ö†Ô∏è Rate limit alcanzado (total={self._rate_limit_hits}). "
                f"Esperando {wait_time}s antes de reintentar..."
            )
            time.sleep(wait_time)
            return True

        return False

    # ==================== M√âTODOS PRINCIPALES ====================

    def chat_with_context(self, user_message: str, conversation_history: list[dict] = None, session_data: dict = None) -> dict:
        """
        Conversaci√≥n natural con GPT manteniendo contexto completo
        MANEJA CUALQUIER TIPO DE SOLICITUD DEL USUARIO
        
        Args:
            user_message: Mensaje actual del usuario
            conversation_history: Historial de mensajes previos
            session_data: Datos de la sesi√≥n actual (productos detectados, precios, etc.)
        
        Returns:
            Dict con respuesta y acciones a realizar
        """
        if not self.is_available():
            # Fallback inteligente sin IA
            return self._intelligent_fallback(user_message, session_data)

        try:
            # Construir historial de conversaci√≥n
            messages = [
                {"role": "system", "content": self._get_conversation_system_prompt()}
            ]

            # Agregar contexto de sesi√≥n si existe
            if session_data:
                context_message = self._build_context_message(session_data)
                messages.append({"role": "system", "content": context_message})

            # Agregar historial de conversaci√≥n
            if conversation_history:
                messages.extend(conversation_history[-10:])  # √öltimos 10 mensajes

            # Agregar mensaje actual
            messages.append({"role": "user", "content": user_message})

            # Hacer petici√≥n a GPT con reintentos
            result = self._make_request_with_retry(messages, max_tokens=500, temperature=0.7)

            if result:
                # Parsear respuesta para extraer acciones
                parsed = self._parse_gpt_response(result)
                return parsed
            else:
                # Fallback inteligente si falla la petici√≥n
                return self._intelligent_fallback(user_message, session_data)

        except Exception as e:
            logger.error(f"‚ùå Error en chat con contexto: {str(e)}")
            # Siempre retornar algo coherente
            return self._intelligent_fallback(user_message, session_data)

    def _get_conversation_system_prompt(self) -> str:
        """
        Prompt del sistema para conversaci√≥n natural
        """
        return """Eres ShrimpBot, el asistente comercial de BGR Export especializado en camarones premium.

TU PERSONALIDAD:
- Profesional pero amigable y conversacional
- Experto en productos de camar√≥n y langostino
- Proactivo en ayudar al cliente
- Usas emojis apropiados (ü¶ê, üí∞, üìä, üìã, ‚ùÑÔ∏è, üåç)
- Mantienes conversaciones naturales y fluidas
- Recuerdas el contexto de la conversaci√≥n

PRODUCTOS DISPONIBLES:
- HOSO (Head On Shell On) - Camar√≥n entero con cabeza
- HLSO (Head Less Shell On) - Sin cabeza, con c√°scara
- P&D IQF (Peeled & Deveined) - Pelado y desvenado individual
- P&D BLOQUE - Pelado y desvenado en bloque
- EZ PEEL - F√°cil pelado
- PuD-EUROPA - Calidad premium para Europa
- PuD-EEUU - Calidad para Estados Unidos
- COOKED - Cocido listo para consumo
- PRE-COCIDO - Pre-cocido para procesamiento
- COCIDO SIN TRATAR - Cocido sin tratamiento adicional

RECONOCIMIENTO DE T√âRMINOS:
- "Cocedero", "Cocido", "Cooked" ‚Üí COOKED, PRE-COCIDO, o COCIDO SIN TRATAR
- "Inteiro", "Entero", "Whole" ‚Üí HOSO o HLSO
- "Colas", "Tails", "Cola" ‚Üí P&D IQF, COOKED (colas peladas)
- "Lagostino", "Vannamei", "Camar√≥n" ‚Üí Todos son v√°lidos
- "CFR", "CIF", "FOB" ‚Üí T√©rminos de flete (detectar destino)

TALLAS: U15, 16/20, 20/30, 21/25, 26/30, 30/40, 31/35, 36/40, 40/50, 41/50, 50/60, 51/60, 60/70, 61/70, 70/80, 71/90

TU OBJETIVO:
Ayudar al cliente a generar cotizaciones y proformas de camar√≥n.

DETECCI√ìN INTELIGENTE DE SOLICITUDES COMPLEJAS:
Cuando el usuario env√≠a un mensaje con m√∫ltiples tallas y productos:
1. Extrae TODAS las tallas mencionadas (ej: 20/30, 30/40, 40/50, 21/25, 31/35, etc.)
2. Agrupa las tallas por tipo de producto si se menciona (ej: "Inteiro" vs "Colas")
3. Detecta el destino si menciona ciudades o t√©rminos CFR/CIF
4. Resume claramente lo que detectaste
5. Pregunta por la informaci√≥n faltante (glaseo, cantidades, confirmaci√≥n de producto)

FLUJO DE CONVERSACI√ìN:
1. Detectar qu√© productos y tallas necesita el cliente
2. Preguntar por glaseo si no lo especific√≥ (10%, 20%, 30%)
3. Preguntar por cantidades si no las especific√≥
4. Confirmar destino si menciona CFR/CIF
5. Confirmar datos antes de generar proforma
6. Preguntar idioma del PDF (Espa√±ol/English)

REGLAS IMPORTANTES:
- Mant√©n respuestas concisas pero completas cuando hay m√∫ltiples tallas
- Si detectas m√∫ltiples tallas, lista todas claramente
- Si falta informaci√≥n cr√≠tica (glaseo, producto espec√≠fico), pregunta de forma natural
- Confirma siempre antes de generar proforma
- Usa lenguaje natural, no rob√≥tico
- Si el usuario menciona "Cocedero" o "Cocido", ofrece las opciones: COOKED, PRE-COCIDO, COCIDO SIN TRATAR

FORMATO DE RESPUESTA:
Responde en formato JSON con esta estructura:
{
    "response": "Tu respuesta natural al usuario",
    "action": "detect_products|ask_glaseo|ask_product_type|ask_language|generate_proforma|none",
    "data": {
        "products": [...],
        "glaseo": 20,
        "destination": "Lisboa",
        "language": "es",
        ...
    }
}

EJEMPLOS:
Usuario: "Hola"
Respuesta: {
    "response": "¬°Hola! üëã Soy ShrimpBot de BGR Export. ¬øEn qu√© puedo ayudarte hoy? ü¶ê",
    "action": "none",
    "data": {}
}

Usuario: "Necesito precios de HLSO 16/20"
Respuesta: {
    "response": "¬°Perfecto! HLSO 16/20 es una excelente opci√≥n. ‚ùÑÔ∏è ¬øQu√© glaseo necesitas? (10%, 20% o 30%)",
    "action": "ask_glaseo",
    "data": {"products": [{"product": "HLSO", "size": "16/20"}]}
}

Usuario: "Necesito precios Lagostino Cocedero CFR Lisboa: Inteiro 20/30, 30/40, 40/50. Colas 21/25, 31/35, 36/40, 41/50"
Respuesta: {
    "response": "ü¶ê ¬°Hola! Entiendo que necesitas cotizaci√≥n para langostino cocido CFR Lisboa.\n\nHe detectado las siguientes tallas:\nüìè Inteiro: 20/30, 30/40, 40/50\nüìè Colas: 21/25, 31/35, 36/40, 41/50\n\nPara generar tu cotizaci√≥n necesito confirmar:\n1Ô∏è‚É£ ¬øQu√© porcentaje de glaseo necesitas? (10%, 20%, 30%)\n2Ô∏è‚É£ ¬øCantidad aproximada por talla?\n3Ô∏è‚É£ ¬øConfirmas destino Lisboa?\n\nüí° Nuestros productos cocidos disponibles:\n- COOKED\n- PRE-COCIDO\n- COCIDO SIN TRATAR\n\n¬øCon cu√°l deseas cotizar? ü¶ê",
    "action": "ask_product_type",
    "data": {
        "sizes_inteiro": ["20/30", "30/40", "40/50"],
        "sizes_colas": ["21/25", "31/35", "36/40", "41/50"],
        "destination": "Lisboa",
        "product_category": "cocido"
    }
}"""

    def _build_context_message(self, session_data: dict) -> str:
        """
        Construye mensaje de contexto con datos de la sesi√≥n
        """
        context_parts = ["CONTEXTO DE LA SESI√ìN ACTUAL:"]

        if session_data.get('products'):
            products_list = ", ".join([f"{p['product']} {p['size']}" for p in session_data['products']])
            context_parts.append(f"- Productos detectados: {products_list}")

        if session_data.get('glaseo_percentage'):
            context_parts.append(f"- Glaseo especificado: {session_data['glaseo_percentage']}%")

        if session_data.get('destination'):
            context_parts.append(f"- Destino: {session_data['destination']}")

        if session_data.get('language'):
            context_parts.append(f"- Idioma preferido: {session_data['language']}")

        if session_data.get('state'):
            context_parts.append(f"- Estado actual: {session_data['state']}")

        return "\n".join(context_parts)

    def _parse_gpt_response(self, response: str) -> dict:
        """
        Parsea la respuesta de GPT para extraer JSON con validaci√≥n de schema
        """
        try:
            # Intentar parsear como JSON
            # Buscar JSON en la respuesta
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)

                # Validar schema: campos requeridos
                required_fields = ['response', 'action', 'data']
                if not all(field in parsed for field in required_fields):
                    logger.warning(f"‚ö†Ô∏è Respuesta GPT incompleta. Campos presentes: {parsed.keys()}")
                    # Intentar recuperar con defaults
                    return self._get_default_response(
                        text=parsed.get('response', response),
                        action=parsed.get('action', 'none'),
                        data=parsed.get('data', {})
                    )

                # Validar action: debe ser uno de los permitidos
                valid_actions = [
                    'detect_products', 'ask_glaseo', 'ask_language', 'ask_product_type',
                    'generate_proforma', 'greeting', 'none', 'help',
                    'audio_info', 'product_list', 'price_inquiry', 'thanks',
                    'goodbye', 'size_detected', 'session_context', 'general_inquiry',
                    'emergency_fallback', 'empty_message', 'audio_transcription_failed',
                    'audio_processing_error'
                ]
                if parsed['action'] not in valid_actions:
                    logger.warning(f"‚ö†Ô∏è Action inv√°lida: '{parsed['action']}'. Usando 'none'")
                    parsed['action'] = 'none'

                # Validar tipos de datos
                if not isinstance(parsed['response'], str):
                    logger.warning(f"‚ö†Ô∏è Campo 'response' no es string: {type(parsed['response'])}")
                    parsed['response'] = str(parsed['response'])

                if not isinstance(parsed['data'], dict):
                    logger.warning(f"‚ö†Ô∏è Campo 'data' no es dict: {type(parsed['data'])}")
                    parsed['data'] = {}

                return parsed
            else:
                # Si no hay JSON, retornar respuesta como texto
                logger.warning("‚ö†Ô∏è No se encontr√≥ JSON en la respuesta de GPT")
                return self._get_default_response(text=response)

        except json.JSONDecodeError as e:
            # Si falla el parseo, retornar respuesta como texto
            logger.error(f"‚ùå Error parseando JSON de GPT: {str(e)}")
            return self._get_default_response(text=response)
        except Exception as e:
            logger.error(f"‚ùå Error inesperado en _parse_gpt_response: {str(e)}")
            return self._get_default_response(text=response)

    def _get_default_response(self, text: str = None, action: str = "none", data: dict = None) -> dict:
        """
        Genera una respuesta por defecto v√°lida cuando falla el parsing
        """
        return {
            "response": text if text else "ü¶ê ¬øEn qu√© puedo ayudarte hoy?",
            "action": action,
            "data": data if data is not None else {}
        }

    def _make_request(self, messages: list[dict], max_tokens: int = 300, temperature: float = 0.3, use_cache: bool = True) -> str | None:
        """
        Hace una petici√≥n directa a la API de OpenAI con cach√© y rate limiting.

        Args:
            messages: Lista de mensajes para la conversaci√≥n
            max_tokens: N√∫mero m√°ximo de tokens en la respuesta
            temperature: Temperatura para generaci√≥n (0.0-1.0)
            use_cache: Si debe usar el sistema de cach√© (default: True)

        Returns:
            Respuesta de la API o None si falla
        """
        if not self.is_available():
            logger.warning("‚ö†Ô∏è OpenAI API key no configurada")
            return None

        try:
            # Generar clave de cach√©
            if use_cache:
                prompt_text = json.dumps(messages, sort_keys=True)
                params = {'max_tokens': max_tokens, 'temperature': temperature, 'model': self.model}
                cache_key = self._generate_cache_key(prompt_text, params)

                # Intentar obtener del cach√©
                cached_response = self._get_from_cache(cache_key)
                if cached_response:
                    return cached_response

            # Preparar petici√≥n
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            data = {
                "model": self.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            }

            # Hacer petici√≥n con timeout
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )

            # Manejar respuesta exitosa
            if response.status_code == 200:
                result = response.json()
                response_text = result["choices"][0]["message"]["content"].strip()

                # Guardar en cach√© si est√° habilitado
                if use_cache:
                    self._save_to_cache(cache_key, response_text)

                return response_text

            # Manejar rate limiting
            elif response.status_code == 429:
                logger.warning(f"‚ö†Ô∏è Rate limit alcanzado. Headers: {response.headers}")
                # El m√©todo _make_request_with_retry se encarga de manejar esto
                return None

            # Otros errores
            else:
                error_detail = ""
                try:
                    error_json = response.json()
                    error_detail = error_json.get('error', {}).get('message', '')
                except:
                    error_detail = response.text[:200]

                logger.error(
                    f"‚ùå Error API OpenAI: {response.status_code} | "
                    f"Detalle: {error_detail}"
                )
                return None

        except requests.exceptions.Timeout:
            logger.error("‚ùå Timeout en petici√≥n a OpenAI (30s)")
            return None
        except requests.exceptions.ConnectionError as e:
            logger.error(f"‚ùå Error de conexi√≥n con OpenAI: {str(e)}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error de red en petici√≥n OpenAI: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error inesperado en petici√≥n OpenAI: {str(e)}")
            return None

    def analyze_user_intent(self, message: str, context: dict = None) -> dict:
        """
        Analiza la intenci√≥n del usuario usando GPT-4o mini
        """
        if not self.is_available():
            # Fallback con an√°lisis b√°sico de patrones
            return self._basic_intent_analysis(message)

        try:
            system_prompt = """
Analiza solicitudes de exportaci√≥n de camar√≥n/langostino y extrae informaci√≥n estructurada.

PRODUCTOS V√ÅLIDOS: HOSO, HLSO, P&D IQF, P&D BLOQUE, EZ PEEL, PuD-EUROPA, PuD-EEUU, COOKED, PRE-COCIDO, COCIDO SIN TRATAR
TALLAS V√ÅLIDAS: U15, 16/20, 20/30, 21/25, 26/30, 30/40, 31/35, 36/40, 40/50, 41/50, 50/60, 51/60, 60/70, 61/70, 70/80, 71/90

RECONOCIMIENTO DE T√âRMINOS (espa√±ol/portugu√©s/ingl√©s):
IMPORTANTE: "Cocedero"/"Cocido" es la CALIDAD/PROCESAMIENTO, NO el producto final.

COMBINACIONES V√ÅLIDAS:
- "Inteiro Cocedero" / "Entero Cocido" ‚Üí Solicitar aclaraci√≥n (¬øHOSO cocido? ¬øCOOKED entero?)
- "Colas Cocedero" / "Colas Cocidas" ‚Üí COOKED (colas peladas cocidas)
- Solo "Cocedero" sin especificar ‚Üí Solicitar tipo: COOKED, PRE-COCIDO, COCIDO SIN TRATAR
- "Inteiro" solo (sin cocedero) ‚Üí HOSO (camar√≥n entero crudo)
- "Colas" solo (sin cocedero) ‚Üí P&D IQF (colas peladas crudas)

OTROS T√âRMINOS:
- "Lagostino", "Vannamei", "Camar√≥n", "Shrimp" ‚Üí Todos v√°lidos
- "CFR [ciudad]", "CIF [ciudad]" ‚Üí Detectar destino y marcar flete_solicitado: true
- "Contenedor" ‚Üí Solicitud de cotizaci√≥n grande

EXTRAE (valores exactos o null):
- Intent: pricing|proforma|product_info|greeting|help|other
- Product: nombre exacto del producto (o null si necesita especificar entre COOKED/PRE-COCIDO/COCIDO SIN TRATAR)
- Size: talla exacta (o array de tallas si menciona m√∫ltiples)
- Glaseo: porcentaje (0, 10, 20, 30) ‚Üí convertir a factor decimal
  * 0% = null (sin glaseo, CFR simple)
  * 10% = 0.90
  * 20% = 0.80
  * 30% = 0.70
- Destination: ciudad/pa√≠s si menciona "CFR [lugar]", "CIF [lugar]", "flete a [lugar]", "env√≠o a [lugar]"
- Flete: valor num√©rico si menciona "flete 0.30", "freight $0.25", etc.
- Cantidad: n√∫mero + unidad (ej: "5000 kg", "10000 lb")
- Cliente: nombre si menciona "cliente [nombre]", "para [nombre]"
- Language: "es" (espa√±ol) o "en" (ingl√©s)
- Multiple_sizes: true si detecta m√∫ltiples tallas en el mensaje

REGLAS IMPORTANTES:
1. Si menciona tallas (ej: 20/30, 21/25) ‚Üí intent: "proforma" (incluso si empieza con saludo)
2. Si menciona CFR/CIF sin glaseo ‚Üí flete_solicitado: true, glaseo_factor: null (c√°lculo CFR simple)
3. Si menciona CFR/CIF con glaseo (ej: "CFR con 15%") ‚Üí flete_solicitado: true, glaseo_factor: 0.85
4. Si menciona "Cocedero" + "Inteiro" ‚Üí multiple_presentations: true, needs_product_type: true, clarification_needed
5. Si menciona "Cocedero" + "Colas" ‚Üí product: "COOKED" (colas cocidas)
6. Si menciona solo "Cocedero" ‚Üí product: null, needs_product_type: true
7. Si detecta m√∫ltiples tallas ‚Üí multiple_sizes: true, listar todas en array
8. Si detecta "Inteiro" y "Colas" en el mismo mensaje ‚Üí separar tallas: sizes_inteiro y sizes_colas
9. NO asumir valores por defecto - extraer solo lo que el usuario dice expl√≠citamente
10. CFR sin glaseo = Precio FOB + Flete (simple)
11. CFR con glaseo = Precio FOB + Glaseo + Flete (completo)

EJEMPLOS:
Input: "HLSO 16/20 con 20% glaseo"
Output: {intent: "proforma", product: "HLSO", size: "16/20", glaseo_factor: 0.80, destination: null, confidence: 0.9}

Input: "Buenas Tardes. Necesito precios Lagostino Cocedero CFR Lisboa: Inteiro 20/30, 30/40. Colas 21/25, 31/35"
Output: {intent: "proforma", product: null, needs_product_type: true, product_category: "cocido", sizes_inteiro: ["20/30", "30/40"], sizes_colas: ["21/25", "31/35"], destination: "Lisboa", flete_solicitado: true, glaseo_factor: null, multiple_sizes: true, multiple_presentations: true, clarification_needed: "Cliente solicita 'Inteiro Cocedero' y 'Colas Cocedero'. Confirmar si desea productos cocidos o crudos para cada presentaci√≥n.", confidence: 0.95}

Input: "Precio CFR Houston HLSO 16/20"
Output: {intent: "proforma", product: "HLSO", size: "16/20", destination: "Houston", flete_solicitado: true, glaseo_factor: null, confidence: 0.9}

Input: "Precio CFR Houston HLSO 16/20 con 15% glaseo"
Output: {intent: "proforma", product: "HLSO", size: "16/20", destination: "Houston", flete_solicitado: true, glaseo_factor: 0.85, glaseo_percentage: 15, confidence: 0.95}

Input: "Necesito precios CFR Lisboa Inteiro 0% 20/30, 30/40"
Output: {intent: "proforma", product: null, needs_product_type: true, sizes: ["20/30", "30/40"], destination: "Lisboa", flete_solicitado: true, glaseo_factor: null, glaseo_percentage: 0, confidence: 0.9}

Input: "Precio DDP Houston con flete 0.30"
Output: {intent: "proforma", destination: "Houston", flete_custom: 0.30, is_ddp: true, usar_libras: false, confidence: 0.9}

Input: "Hola, necesito cotizaci√≥n"
Output: {intent: "greeting", product: null, wants_proforma: true, confidence: 0.8}

Responde SOLO en JSON:
{
  "intent": "...",
  "product": null | "...",
  "size": null | "...",
  "sizes": null | [...],
  "sizes_inteiro": null | [...],
  "sizes_colas": null | [...],
  "multiple_sizes": false,
  "multiple_presentations": false,
  "needs_product_type": false,
  "product_category": null | "cocido",
  "clarification_needed": null | "...",
  "glaseo_factor": null | number,
  "glaseo_percentage": null | number,
  "destination": null | "...",
  "flete_custom": null | number,
  "precio_base_custom": null | number,
  "flete_solicitado": false,
  "is_ddp": false,
  "usar_libras": false,
  "cantidad": null | "...",
  "cliente_nombre": null | "...",
  "wants_proforma": false,
  "language": "es",
  "confidence": 0.0-1.0
}
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Mensaje: '{message}'"}
            ]

            result = self._make_request(messages, max_tokens=300, temperature=0.3)

            if result:
                # Intentar parsear como JSON
                try:
                    parsed_result = json.loads(result)
                    logger.info(f"ü§ñ An√°lisis OpenAI: {parsed_result}")
                    return parsed_result
                except json.JSONDecodeError:
                    logger.error(f"‚ùå Error parseando JSON de OpenAI: {result}")
                    return {"intent": "unknown", "confidence": 0}
            else:
                return {"intent": "unknown", "confidence": 0}

        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis OpenAI: {str(e)}")
            return {"intent": "unknown", "confidence": 0}

    def generate_smart_response(self, user_message: str, context: dict, price_data: dict = None) -> str | None:
        """
        Genera una respuesta inteligente y personalizada
        """
        if not self.is_available():
            return None

        try:
            # Construir contexto para GPT
            context_info = f"""
Usuario: {user_message}
Estado actual: {context.get('state', 'unknown')}
Datos de sesi√≥n: {context.get('data', {})}
"""

            if price_data:
                context_info += f"\nDatos de precio disponibles: {price_data}"

            system_prompt = """
Eres ShrimpBot, el asistente comercial de BGR Export especializado en camarones premium. Tu objetivo principal es ayudar a los clientes a crear proformas y cotizaciones.

PERSONALIDAD:
- Profesional pero amigable y proactivo
- Experto comercial en productos de camar√≥n
- Usa emojis apropiados (ü¶ê, üí∞, üìä, üìã, etc.)
- Siempre gu√≠a hacia la creaci√≥n de proformas
- Enfocado en cerrar ventas y generar cotizaciones

PRODUCTOS DISPONIBLES:
- HOSO (Head On Shell On) - Camar√≥n entero con cabeza
- HLSO (Head Less Shell On) - Sin cabeza, con c√°scara  
- P&D IQF (Peeled & Deveined) - Pelado y desvenado individual
- P&D BLOQUE - Pelado y desvenado en bloque
- PuD-EUROPA - Calidad premium para Europa
- EZ PEEL - F√°cil pelado
- PuD-EEUU - Calidad para Estados Unidos
- COOKED - Cocido listo para consumo
- PRE-COCIDO - Pre-cocido
- COCIDO SIN TRATAR - Cocido sin procesar

TALLAS DISPONIBLES: U15, 16/20, 20/30, 21/25, 26/30, 30/40, 31/35, 36/40, 40/50, 41/50, 50/60, 51/60, 60/70, 61/70, 70/80, 71/90

INSTRUCCIONES CLAVE:
- Si el usuario ya especific√≥ PRODUCTO y TALLA: NO pidas m√°s informaci√≥n, confirma que generas la proforma
- Para saludos: m√°ximo 100 caracteres
- Para preguntas r√°pidas: m√°ximo 150 caracteres
- Para confirmaciones (con producto + talla): m√°ximo 200 caracteres
- Si necesitas listar m√∫ltiples opciones: puedes usar hasta 300 caracteres
- Si tienes producto y talla, di: "¬°Perfecto! Generando tu proforma de [producto] [talla]..."
- NO pidas cantidad si ya tienes producto y talla - genera la proforma directamente

REGLA CR√çTICA: Si detectas producto + talla en el mensaje, NUNCA pidas m√°s informaci√≥n. Genera la proforma directamente.

OBJETIVO: Generar proformas inmediatamente cuando tengas datos suficientes.
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": context_info}
            ]

            result = self._make_request(messages, max_tokens=200, temperature=0.7)

            if result:
                # Limpiar emojis problem√°ticos que pueden causar errores de codificaci√≥n
                cleaned_result = self._clean_problematic_emojis(result)
                logger.info(f"ü§ñ Respuesta generada por OpenAI: {cleaned_result}")
                return cleaned_result
            else:
                return None

        except Exception as e:
            logger.error(f"‚ùå Error generando respuesta OpenAI: {str(e)}")
            return None

    def enhance_price_explanation(self, price_data: dict) -> str | None:
        """
        Mejora la explicaci√≥n de precios usando IA
        """
        if not self.is_available() or not price_data:
            return None

        try:
            system_prompt = """
Eres un experto en explicar precios de camar√≥n de manera clara y profesional.

Toma los datos de precio y genera una explicaci√≥n breve y clara que incluya:
- Destacar el producto y talla
- Mencionar los diferentes tipos de precio (base, FOB, glaseo, final)
- Usar emojis apropiados
- M√°ximo 150 caracteres
- Tono profesional pero amigable

Formato de respuesta: texto directo sin JSON.
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Datos de precio: {price_data}"}
            ]

            result = self._make_request(messages, max_tokens=100, temperature=0.5)

            if result:
                return result
            else:
                return None

        except Exception as e:
            logger.error(f"‚ùå Error mejorando explicaci√≥n de precio: {str(e)}")
            return None

    # ==================== M√âTODOS ESPECIALIZADOS POR RESPONSABILIDAD ====================

    def _get_base_context(self) -> str:
        """
        Contexto base com√∫n para todos los prompts
        Reutilizable para evitar duplicaci√≥n
        """
        return """Eres ShrimpBot, el asistente comercial de BGR Export especializado en camarones premium.

PRODUCTOS: HOSO, HLSO, P&D IQF, P&D BLOQUE, EZ PEEL, PuD-EUROPA, PuD-EEUU, COOKED
TALLAS: U15, 16/20, 20/30, 21/25, 26/30, 30/40, 31/35, 36/40, 40/50, 41/50, 50/60, 51/60, 60/70, 61/70, 70/80, 71/90

PERSONALIDAD: Profesional, amigable, usa emojis apropiados (ü¶ê, üí∞, üìä, üìã)"""

    def generate_greeting_response(self, user_name: str = None) -> str | None:
        """
        Genera respuesta espec√≠fica para saludos
        M√©todo especializado, r√°pido y eficiente
        """
        if not self.is_available():
            return None

        try:
            base = self._get_base_context()

            user_context = f"Usuario: {user_name}" if user_name else "Usuario nuevo"

            system_prompt = f"""{base}

TAREA: Responder a un saludo de forma amigable y directa.
L√çMITE: M√°ximo 100 caracteres.
OBJETIVO: Saludar Y preguntar qu√© producto necesita.

{user_context}
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "Saluda al usuario"}
            ]

            result = self._make_request(messages, max_tokens=50, temperature=0.8)

            if result:
                logger.info(f"üí¨ Saludo generado: {result[:50]}...")
                return self._clean_problematic_emojis(result)

            return None

        except Exception as e:
            logger.error(f"‚ùå Error generando saludo: {str(e)}")
            return None

    def generate_confirmation_response(self, product: str, size: str, additional_info: dict = None) -> str | None:
        """
        Genera confirmaci√≥n para generaci√≥n de proforma
        M√©todo especializado para confirmaciones
        """
        if not self.is_available():
            return None

        try:
            base = self._get_base_context()

            context = f"Producto: {product}, Talla: {size}"
            if additional_info:
                if additional_info.get('glaseo_percentage'):
                    context += f", Glaseo: {additional_info['glaseo_percentage']}%"
                if additional_info.get('destination'):
                    context += f", Destino: {additional_info['destination']}"

            system_prompt = f"""{base}

TAREA: Confirmar que vas a generar la proforma.
L√çMITE: M√°ximo 200 caracteres.
TONO: Entusiasta y profesional.
FORMATO: "¬°Perfecto! Generando tu proforma de [producto] [talla]..."

Datos detectados: {context}
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "Confirma generaci√≥n de proforma"}
            ]

            result = self._make_request(messages, max_tokens=80, temperature=0.7)

            if result:
                logger.info(f"‚úÖ Confirmaci√≥n generada para {product} {size}")
                return self._clean_problematic_emojis(result)

            return None

        except Exception as e:
            logger.error(f"‚ùå Error generando confirmaci√≥n: {str(e)}")
            return None

    def generate_question_response(self, question_type: str, context: dict = None) -> str | None:
        """
        Genera preguntas espec√≠ficas seg√∫n el tipo

        Args:
            question_type: 'glaseo', 'language', 'destination', 'product', 'size'
            context: Contexto adicional opcional
        """
        if not self.is_available():
            return None

        try:
            base = self._get_base_context()

            question_prompts = {
                'glaseo': {
                    'task': 'Preguntar qu√© porcentaje de glaseo necesita',
                    'options': 'Opciones: 10%, 20%, 30%',
                    'limit': 150
                },
                'language': {
                    'task': 'Preguntar en qu√© idioma quiere la proforma',
                    'options': 'Opciones: Espa√±ol o English',
                    'limit': 150
                },
                'destination': {
                    'task': 'Preguntar a qu√© destino necesita env√≠o',
                    'options': 'Menciona destinos comunes: Houston, Miami, China, etc.',
                    'limit': 150
                },
                'product': {
                    'task': 'Preguntar qu√© tipo de producto necesita',
                    'options': 'Opciones populares: HLSO (sin cabeza), HOSO (con cabeza), P&D IQF (pelado)',
                    'limit': 200
                },
                'size': {
                    'task': 'Preguntar qu√© talla necesita',
                    'options': 'Menciona tallas populares: 16/20, 21/25, 26/30, 31/35',
                    'limit': 150
                }
            }

            if question_type not in question_prompts:
                logger.warning(f"‚ö†Ô∏è Tipo de pregunta desconocido: {question_type}")
                return None

            q_config = question_prompts[question_type]
            context_str = f"\nContexto adicional: {context}" if context else ""

            system_prompt = f"""{base}

TAREA: {q_config['task']}
{q_config['options']}
L√çMITE: M√°ximo {q_config['limit']} caracteres.
TONO: Amigable y directo.{context_str}
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Pregunta sobre {question_type}"}
            ]

            result = self._make_request(messages, max_tokens=60, temperature=0.7)

            if result:
                logger.info(f"‚ùì Pregunta generada: tipo={question_type}")
                return self._clean_problematic_emojis(result)

            return None

        except Exception as e:
            logger.error(f"‚ùå Error generando pregunta: {str(e)}")
            return None

    def _estimate_tokens(self, text: str) -> int:
        """
        Estima el n√∫mero de tokens en un texto
        Regla simple: ~4 caracteres = 1 token en espa√±ol
        ~3 caracteres = 1 token en ingl√©s
        """
        # Detectar idioma aproximadamente
        spanish_chars = sum(1 for c in text if c in '√°√©√≠√≥√∫√±¬ø¬°')
        ratio = 4 if spanish_chars > 3 else 3.5

        return int(len(text) / ratio)

    def _log_token_usage(self, prompt: str, response: str, method_name: str):
        """
        Registra el uso de tokens para m√©tricas
        """
        prompt_tokens = self._estimate_tokens(prompt)
        response_tokens = self._estimate_tokens(response) if response else 0
        total_tokens = prompt_tokens + response_tokens

        # Calcular costo aproximado (GPT-3.5-turbo: $0.0015/1K prompt + $0.002/1K completion)
        cost = (prompt_tokens * 0.0015 / 1000) + (response_tokens * 0.002 / 1000)

        logger.info(
            f"üìä Tokens usados en {method_name}: "
            f"Prompt={prompt_tokens}, Response={response_tokens}, "
            f"Total={total_tokens}, Costo‚âà${cost:.6f}"
        )

    # ==================== SISTEMA DE EJEMPLOS DIN√ÅMICOS (FEW-SHOT LEARNING) ====================

    def _get_relevant_examples(self, context_type: str, detected_data: dict = None) -> str:
        """
        Selecciona ejemplos relevantes basados en el contexto actual
        Few-shot learning din√°mico para mejorar respuestas

        Args:
            context_type: 'greeting', 'product_query', 'proforma_complete', 'missing_info'
            detected_data: Datos detectados del usuario (productos, tallas, etc.)

        Returns:
            String con ejemplos relevantes formateados
        """
        examples = {
            'greeting': """
EJEMPLO RELEVANTE:
Usuario: "Hola"
Asistente: "¬°Hola! üëã Soy ShrimpBot. ¬øQu√© producto necesitas? ü¶ê"
""",
            'product_query': """
EJEMPLO RELEVANTE:
Usuario: "Necesito HLSO"
Asistente: "¬°Perfecto! HLSO es muy popular. ¬øQu√© talla necesitas? üìä Tenemos 16/20, 21/25, 26/30..."
""",
            'proforma_complete': """
EJEMPLO RELEVANTE:
Usuario: "HLSO 16/20 con glaseo 20%"
Asistente: "¬°Excelente! ü¶ê Generando proforma de HLSO 16/20 con glaseo 20%. Un momento..."
""",
            'missing_glaseo': """
EJEMPLO RELEVANTE:
Usuario: "HLSO 16/20"
Asistente: "¬°Perfecto! HLSO 16/20. ¬øQu√© glaseo necesitas? (10%, 20% o 30%) ‚ùÑÔ∏è"
""",
            'missing_language': """
EJEMPLO RELEVANTE:
Usuario: [tiene producto y talla]
Asistente: "Excelente. ¬øEn qu√© idioma quieres la proforma? üåê (Espa√±ol/English)"
""",
            'missing_destination': """
EJEMPLO RELEVANTE:
Usuario: "Con flete"
Asistente: "¬øA qu√© destino lo necesitas? üåç Houston, Miami, China..."
"""
        }

        # Seleccionar ejemplo base
        example = examples.get(context_type, "")

        # Personalizar ejemplo si hay datos detectados
        if detected_data:
            if detected_data.get('product') and detected_data.get('size'):
                prod = detected_data['product']
                size = detected_data['size']
                example = f"""
EJEMPLO SIMILAR A TU CASO:
Usuario: "{prod} {size}"
Asistente: "¬°Perfecto! {prod} {size} es excelente. Generando tu proforma... ü¶ê"
"""

        return example

    def _build_contextual_system_prompt(self, base_prompt: str, context_type: str, detected_data: dict = None) -> str:
        """
        Construye un prompt con ejemplos din√°micos relevantes

        Args:
            base_prompt: Prompt base del sistema
            context_type: Tipo de contexto actual
            detected_data: Datos detectados

        Returns:
            Prompt enriquecido con ejemplos relevantes
        """
        relevant_examples = self._get_relevant_examples(context_type, detected_data)

        return f"""{base_prompt}

{relevant_examples}

APLICA EL MISMO ESTILO Y TONO QUE EN LOS EJEMPLOS."""

    def transcribe_audio(self, audio_file_path: str, language: str = 'es') -> str | None:
        """
        Transcribe audio usando OpenAI Whisper con manejo robusto
        Soporta m√∫ltiples idiomas y formatos de audio
        
        Args:
            audio_file_path: Ruta al archivo de audio
            language: C√≥digo de idioma (es, en, etc.) - None para detecci√≥n autom√°tica
        
        Returns:
            Texto transcrito o None si falla
        """
        if not self.is_available():
            logger.warning("‚ö†Ô∏è OpenAI no disponible para transcripci√≥n de audio")
            return None

        try:
            # Verificar que el archivo existe
            if not os.path.exists(audio_file_path):
                logger.error(f"‚ùå Archivo de audio no encontrado: {audio_file_path}")
                return None

            # Verificar tama√±o del archivo (m√°ximo 25MB para Whisper)
            file_size = os.path.getsize(audio_file_path)
            if file_size > 25 * 1024 * 1024:  # 25MB
                logger.error(f"‚ùå Archivo de audio muy grande: {file_size / (1024*1024):.2f}MB")
                return None

            logger.info(f"üé§ Transcribiendo audio: {audio_file_path} ({file_size / 1024:.2f}KB)")

            headers = {
                "Authorization": f"Bearer {self.api_key}"
            }

            with open(audio_file_path, 'rb') as audio_file:
                files = {
                    'file': audio_file,
                    'model': (None, self.whisper_model)
                }

                # Agregar idioma solo si se especifica
                if language:
                    files['language'] = (None, language)

                # Intentar transcripci√≥n con timeout extendido
                response = requests.post(
                    f"{self.base_url}/audio/transcriptions",
                    headers=headers,
                    files=files,
                    timeout=60  # Timeout m√°s largo para archivos grandes
                )

            if response.status_code == 200:
                result = response.json()
                transcription = result.get('text', '').strip()

                if transcription:
                    logger.info(f"‚úÖ Audio transcrito exitosamente: '{transcription[:100]}...'")
                    return transcription
                else:
                    logger.warning("‚ö†Ô∏è Transcripci√≥n vac√≠a")
                    return None
            else:
                logger.error(f"‚ùå Error API Whisper: {response.status_code} - {response.text}")
                return None

        except requests.exceptions.Timeout:
            logger.error("‚ùå Timeout en transcripci√≥n de audio")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error de red en transcripci√≥n: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error inesperado en transcripci√≥n de audio: {str(e)}")
            return None

    def detect_multiple_products(self, message: str) -> list[dict]:
        """
        Detecta m√∫ltiples productos en un mensaje
        Retorna lista de diccionarios con producto y talla
        """
        if not message:
            return []

        message_upper = message.upper()
        products_found = []

        # Patrones para productos
        product_patterns = {
            'HOSO': r'\bHOSO\b',
            'HLSO': r'\bHLSO\b',
            'P&D IQF': r'\b(?:P&D|PYD|P\s*&\s*D)\s*(?:IQF|TAIL\s*OFF)?\b',
            'P&D BLOQUE': r'\b(?:P&D|PYD)\s*(?:BLOQUE|BLOCK)\b',
            'EZ PEEL': r'\b(?:EZ\s*PEEL|EZPEEL)\b',  # Usar "EZ PEEL" con espacio como en Excel
        }

        # Buscar todas las l√≠neas del mensaje
        lines = message_upper.split('\n')

        for line in lines:
            line = line.strip()
            if not line or len(line) < 5:
                continue

            # Buscar talla en la l√≠nea (formato XX/XX o XX-XX)
            size_match = re.search(r'(\d+)[/-](\d+)', line)
            if not size_match:
                continue

            size = f"{size_match.group(1)}/{size_match.group(2)}"

            # Buscar producto en la l√≠nea
            product_found = None
            for product_name, pattern in product_patterns.items():
                if re.search(pattern, line):
                    product_found = product_name
                    break

            # Si no se encontr√≥ producto espec√≠fico, intentar inferir
            if not product_found:
                # Si tiene "BLOCK" o "BLOQUE", es P&D BLOQUE
                if 'BLOCK' in line or 'BLOQUE' in line:
                    product_found = 'P&D BLOQUE'
                # Si tiene "IQF", es P&D IQF
                elif 'IQF' in line:
                    product_found = 'P&D IQF'

            if product_found and size:
                products_found.append({
                    'product': product_found,
                    'size': size,
                    'line': line
                })

        return products_found

    def _basic_intent_analysis(self, message: str) -> dict:
        """
        An√°lisis b√°sico de intenciones sin IA como fallback
        IMPORTANTE: Detectar cotizaciones ANTES que saludos para evitar falsos positivos
        """
        message_lower = message.lower().strip()

        # PRIMERO: Detectar si hay tallas (fuerte indicador de cotizaci√≥n)
        has_size = bool(re.search(r'\b\d+/\d+\b', message_lower))
        
        # SEGUNDO: Detectar t√©rminos de cotizaci√≥n/precio
        proforma_keywords = [
            'proforma', 'cotizacion', 'cotizar', 'quote', 'precio', 'precios',
            'necesito', 'quiero', 'contenedor', 'cfr', 'cif', 'fob',
            'cocedero', 'cocido', 'lagostino', 'vannamei', 'inteiro', 'colas'
        ]
        has_quote_keywords = any(keyword in message_lower for keyword in proforma_keywords)
        
        # Si tiene tallas O t√©rminos de cotizaci√≥n, NO es solo un saludo
        # Continuar con el an√°lisis de cotizaci√≥n
        is_likely_quote = has_size or has_quote_keywords
        
        # Patrones de saludo (con l√≠mites de palabra para evitar falsos positivos)
        # SOLO considerar saludo si NO tiene indicadores de cotizaci√≥n
        greeting_patterns = [r'\bhola\b', r'\bhello\b', r'\bhi\b', r'\bbuenos\b', r'\bbuenas\b',
                           r'\bcomo estas\b', r'\bque tal\b', r'\bq haces\b']
        has_greeting = any(re.search(pattern, message_lower) for pattern in greeting_patterns)
        
        if has_greeting and not is_likely_quote:
            return {
                "intent": "greeting",
                "product": None,
                "size": None,
                "quantity": None,
                "destination": None,
                "confidence": 0.8,
                "suggested_response": "Responder con saludo amigable y mostrar opciones"
            }

        # Detectar solicitudes de modificaci√≥n de flete
        # IMPORTANTE: Solo detectar cuando hay verbos de modificaci√≥n expl√≠citos
        # NO detectar solicitudes nuevas que incluyen flete
        modify_flete_patterns = [
            r'\bmodifica.*flete', r'\bcambiar.*flete', r'\bactualizar.*flete',
            r'\bnuevo.*flete', r'\botro.*flete', r'\bflete.*diferente',
            r'\bmodify.*freight', r'\bchange.*freight', r'\bupdate.*freight',
        ]

        # Verificar que NO sea una solicitud nueva de cotizaci√≥n/proforma
        new_quote_keywords = ['cotizar', 'cotizacion', 'proforma', 'quote', 'quotation', 'contenedor']
        is_new_quote = any(keyword in message_lower for keyword in new_quote_keywords)

        is_flete_modification = (
            any(re.search(pattern, message_lower) for pattern in modify_flete_patterns) and
            not is_new_quote  # NO es modificaci√≥n si es una solicitud nueva
        )

        if is_flete_modification:
            # Extraer el nuevo valor de flete
            flete_custom = None
            flete_patterns = [
                r'flete\s+a\s+(?:\$\s*)?(\d+\.?\d*)',  # "flete a 0.30"
                r'flete\s*(?:de\s*)?(?:\$\s*)?(\d+\.?\d*)',
                r'(\d+\.?\d*)\s*(?:centavos?\s*)?(?:de\s*)?flete',
                r'con\s*(\d+\.?\d*)\s*(?:de\s*)?flete',
                r'freight\s*(?:of\s*)?(?:\$\s*)?(\d+\.?\d*)',
                r'(\d+\.?\d*)\s*freight',
            ]

            for pattern in flete_patterns:
                match = re.search(pattern, message_lower)
                if match:
                    try:
                        flete_custom = float(match.group(1))
                        break
                    except ValueError:
                        continue

            return {
                "intent": "modify_flete",
                "flete_custom": flete_custom,
                "confidence": 0.9,
                "suggested_response": "Modificar flete y regenerar proforma"
            }

        # Patrones de proforma/cotizaci√≥n (lenguaje natural amplio)
        proforma_patterns = [
            # Palabras clave directas
            'proforma', 'cotizacion', 'cotizar', 'quote', 'precio', 'precios',
            # Verbos de acci√≥n
            'creame', 'crear', 'generar', 'hazme', 'dame', 'quiero', 'necesito',
            # Consultas de precio
            'precio de', 'precio del', 'precio por', 'cuanto cuesta', 'cuanto vale',
            'cuanto es', 'cual es el precio', 'saber el precio', 'conocer el precio',
            # Variaciones comunes
            'cost', 'value', 'rate', 'tarifa', 'valor', 'costo',
            # Frases espec√≠ficas
            'envio a', 'con envio', 'para enviar', 'destino', 'shipping'
        ]

        # Detectar si es una consulta de precio/proforma
        is_price_query = any(pattern in message_lower for pattern in proforma_patterns)

        # Tambi√©n detectar si menciona tallas espec√≠ficas (fuerte indicador)
        has_size = bool(re.search(r'\b\d+/\d+\b', message_lower))

        # Si es consulta de precio O menciona tallas, procesar como proforma
        if is_price_query or has_size:
            # Extraer informaci√≥n b√°sica
            product = None
            size = None
            glaseo_factor = None
            destination = None
            usar_libras = False

            # IMPORTANTE: Detectar si menciona "Cocedero" o "Cocido" como CALIDAD
            # Esto NO define el producto, sino el tipo de procesamiento
            menciona_cocedero = any(term in message_lower for term in ['cocedero', 'cocido', 'cooked', 'cozido'])
            
            # Detectar PRESENTACI√ìN del producto (Inteiro vs Colas)
            menciona_inteiro = any(term in message_lower for term in ['inteiro', 'entero', 'whole'])
            menciona_colas = any(term in message_lower for term in ['colas', 'tails', 'tail', 'cola'])
            
            # L√≥gica inteligente de detecci√≥n:
            # Si menciona "Cocedero" + "Inteiro" ‚Üí NO es COOKED, es solicitud compleja
            # Si menciona "Cocedero" + "Colas" ‚Üí COOKED
            # Si solo menciona "Cocedero" ‚Üí Solicitar especificar
            
            product = None
            needs_clarification = False
            
            if menciona_cocedero:
                if menciona_inteiro and menciona_colas:
                    # Mensaje complejo con m√∫ltiples presentaciones
                    # No asumir producto, dejar que OpenAI lo maneje
                    product = None
                    needs_clarification = True
                elif menciona_inteiro:
                    # "Inteiro Cocedero" ‚Üí Probablemente HOSO cocido (no com√∫n)
                    # Mejor solicitar aclaraci√≥n
                    product = None
                    needs_clarification = True
                elif menciona_colas:
                    # "Colas Cocedero" ‚Üí COOKED (colas cocidas)
                    product = 'COOKED'
                else:
                    # Solo "Cocedero" sin especificar presentaci√≥n
                    product = None
                    needs_clarification = True
            else:
                # No menciona cocedero, usar l√≥gica normal
                product_patterns = {
                    'HLSO': [
                        'sin cabeza', 'hlso', 'head less', 'headless', 'descabezado',
                        'sin cabezas', 'tipo sin cabeza'
                    ],
                    'HOSO': [
                        'con cabeza', 'hoso', 'head on', 'entero', 'completo',
                        'con cabezas', 'tipo con cabeza', 'inteiro', 'whole'
                    ],
                    'P&D IQF': [
                        'p&d iqf', 'pd iqf', 'p&d', 'pelado', 'peeled', 'deveined',
                        'limpio', 'procesado', 'pd', 'p d', 'pelado y desvenado'
                    ],
                    'P&D BLOQUE': [
                        'p&d bloque', 'pd bloque', 'bloque', 'block', 'p&d block',
                        'pd block', 'pelado bloque'
                    ],
                    'PuD-EUROPA': [
                        'pud europa', 'pud-europa', 'europa', 'european', 'europeo'
                    ],
                    'EZ PEEL': [
                        'ez peel', 'ez', 'easy peel', 'facil pelado', 'f√°cil pelado'
                    ],
                    'PuD-EEUU': [
                        'pud eeuu', 'pud-eeuu', 'eeuu', 'usa', 'estados unidos'
                    ],
                    'COOKED': [
                        'cooked', 'cocinado', 'preparado', 'colas', 'tails', 'tail'
                    ],
                    'PRE-COCIDO': [
                        'pre-cocido', 'pre cocido', 'precocido', 'pre-cooked', 'pre cooked'
                    ],
                    'COCIDO SIN TRATAR': [
                        'cocido sin tratar', 'sin tratar', 'untreated', 'natural cocido'
                    ]
                }

                # Buscar coincidencias de productos (orden espec√≠fico para evitar conflictos)
                specific_order = ['COCIDO SIN TRATAR', 'PRE-COCIDO', 'P&D IQF', 'P&D BLOQUE', 'PuD-EUROPA', 'PuD-EEUU', 'EZ PEEL', 'HLSO', 'HOSO', 'COOKED']

                for prod_name in specific_order:
                    if prod_name in product_patterns:
                        patterns = product_patterns[prod_name]
                        if any(pattern in message_lower for pattern in patterns):
                            product = prod_name
                            break

            # Detectar tallas PRIMERO (antes de la l√≥gica de HOSO)
            size_patterns = [
                r'(\d+/\d+)',  # 21/25
                r'(\d+)\s*sobre\s*(\d+)',  # 21 sobre 25
                r'(\d+)\s*-\s*(\d+)',  # 21-25
                r'(\d+)\s+(\d+)'  # 21 25
            ]

            for pattern in size_patterns:
                match = re.search(pattern, message_lower)
                if match:
                    if len(match.groups()) == 1:
                        size = match.group(1)
                    else:
                        size = f"{match.group(1)}/{match.group(2)}"
                    break

            # L√≥gica inteligente: Si no se detect√≥ producto pero hay talla espec√≠fica de HOSO, asumir HOSO
            if not product and size:
                # Tallas que solo existen en HOSO seg√∫n la tabla de precios
                hoso_exclusive_sizes = ['20/30', '30/40', '40/50', '50/60', '60/70', '70/80']
                if size in hoso_exclusive_sizes:
                    product = 'HOSO'

            # NO asumir producto por defecto para otras tallas - el usuario debe especificarlo

            # Detectar glaseo con patrones m√°s amplios (espa√±ol e ingl√©s)
            # IMPORTANTE: Detectar tambi√©n "0%" que significa SIN glaseo
            glaseo_patterns = [
                # Patrones en espa√±ol
                r'(\d+)\s*(?:de\s*)?glaseo',
                r'glaseo\s*(?:de\s*)?(\d+)',
                r'(\d+)\s*%\s*glaseo',
                r'glaseo\s*(\d+)\s*%',
                r'con\s*(\d+)\s*glaseo',
                r'(\d+)\s*porciento\s*glaseo',
                # Patrones adicionales para "al X%" y "al X"
                r'al\s*(\d+)\s*%',  # "al 20%"
                r'al\s*(\d+)(?:\s|$)',  # "al 20" (sin %)
                r'(\d+)\s*%\s*de\s*glaseo',
                r'(\d+)\s*%\s*glaseo',
                # Patr√≥n para "Inteiro 0%" o "0%" solo
                r'(?:inteiro|entero|colas?|tails?)\s+(\d+)\s*%',  # "Inteiro 0%"
                r'^\s*(\d+)\s*%',  # "0%" al inicio
                # Patrones en ingl√©s
                r'(\d+)g?\s*(?:of\s*)?glaze',
                r'glaze\s*(?:of\s*)?(\d+)g?',
                r'(\d+)\s*%\s*glaze',
                r'glaze\s*(\d+)\s*%',
                r'with\s*(\d+)g?\s*glaze',
                r'(\d+)\s*percent\s*glaze',
                r'at\s*(\d+)\s*%',  # "at 20%"
                r'at\s*(\d+)(?:\s|$)'  # "at 20" (sin %)
            ]

            glaseo_percentage_original = None
            for pattern in glaseo_patterns:
                match = re.search(pattern, message_lower)
                if match:
                    glaseo_percentage_original = int(match.group(1))
                    
                    # CASO ESPECIAL: 0% glaseo = Sin glaseo
                    if glaseo_percentage_original == 0:
                        glaseo_factor = None  # No aplicar glaseo
                        logger.info("üîç Detectado 0% glaseo ‚Üí Sin glaseo (CFR simple)")
                    else:
                        # Convertir porcentaje a factor usando f√≥rmula general
                        # Factor = 1 - (percentage / 100)
                        # Ejemplo: 15% glaseo ‚Üí factor = 1 - 0.15 = 0.85
                        glaseo_factor = 1 - (glaseo_percentage_original / 100)
                    break

            # Detectar si menciona DDP (precio que YA incluye flete)
            # DDP = Delivered Duty Paid (precio incluye todo: flete, impuestos, etc.)
            # IMPORTANTE: Si dice DDP, necesitamos el valor del flete para desglosar el precio
            ddp_patterns = [
                r'\bddp\b',  # DDP con l√≠mites de palabra
                r'ddp\s',    # DDP seguido de espacio
                r'\sddp',    # DDP precedido de espacio
                r'precio\s+ddp',
                r'ddp\s+price',
                r'delivered\s+duty\s+paid'
            ]
            menciona_ddp = any(re.search(pattern, message_lower) for pattern in ddp_patterns)

            # Detectar valores num√©ricos de flete
            flete_custom = None
            flete_patterns = [
                r'flete\s*(?:de\s*)?(?:\$\s*)?(\d+\.?\d*)',  # "flete de 0.20", "flete $0.20"
                r'(\d+\.?\d*)\s*(?:centavos?\s*)?(?:de\s*)?flete',  # "0.20 centavos de flete"
                r'con\s*(\d+\.?\d*)\s*(?:de\s*)?flete',  # "con 0.20 de flete"
                r'freight\s*(?:of\s*)?(?:\$\s*)?(\d+\.?\d*)',  # "freight 0.20", "freight $0.20"
                r'(\d+\.?\d*)\s*freight',  # "0.20 freight"
            ]

            # Extraer valor de flete si se menciona
            for pattern in flete_patterns:
                match = re.search(pattern, message_lower)
                if match:
                    try:
                        flete_custom = float(match.group(1))
                        break
                    except ValueError:
                        continue

            # Detectar destinos si se menciona flete, DDP, CFR o CIF
            flete_keywords = ['flete', 'freight', 'envio', 'env√≠o', 'shipping', 'transporte', 'ddp', 'cfr', 'cif', 'c&f']
            menciona_flete = any(keyword in message_lower for keyword in flete_keywords)

            if menciona_flete:
                destination_patterns = {
                    # Ciudades USA
                    'Houston': ['houston', 'houton', 'huston'],
                    'Miami': ['miami', 'maiami', 'florida'],
                    'New York': ['new york', 'nueva york', 'ny', 'newyork'],
                    'Los Angeles': ['los angeles', 'california'],  # Removido 'la' gen√©rico
                    'Chicago': ['chicago', 'chicaco'],
                    'Dallas': ['dallas', 'dalas'],

                    # Ciudades Europa
                    'Lisboa': ['lisboa', 'lisbon', 'portugal'],
                    'Madrid': ['madrid', 'espa√±a', 'spain'],
                    'Barcelona': ['barcelona'],
                    'Paris': ['paris', 'france', 'francia'],
                    'Londres': ['londres', 'london', 'uk', 'reino unido'],
                    'Roma': ['roma', 'rome', 'italy', 'italia'],
                    'Berlin': ['berlin', 'germany', 'alemania'],
                    'Amsterdam': ['amsterdam', 'netherlands', 'holanda'],

                    # Pa√≠ses y regiones
                    'China': ['china', 'beijing', 'shanghai'],
                    'Jap√≥n': ['japon', 'jap√≥n', 'japan', 'tokyo', 'nippon'],
                    'Europa': ['europa', 'europe'],
                    'Brasil': ['brasil', 'brazil', 'sao paulo', 'rio'],
                    'M√©xico': ['mexico', 'm√©xico', 'guadalajara', 'monterrey'],
                    'Canad√°': ['canada', 'toronto', 'vancouver'],
                    'Australia': ['australia', 'sydney', 'melbourne'],
                    'Corea': ['corea', 'korea', 'seoul'],
                    'India': ['india', 'mumbai', 'delhi'],
                    'Tailandia': ['tailandia', 'thailand', 'bangkok'],
                    'Vietnam': ['vietnam', 'ho chi minh'],
                    'Singapur': ['singapur', 'singapore'],
                    'Filipinas': ['filipinas', 'philippines', 'manila'],
                    'Indonesia': ['indonesia', 'jakarta'],
                    'Malasia': ['malasia', 'malaysia', 'kuala lumpur']
                }

                # Buscar destinos USA solo si menciona flete
                for dest_name, patterns in destination_patterns.items():
                    if any(pattern in message_lower for pattern in patterns):
                        if dest_name == 'Houston':
                            usar_libras = False  # Houston es excepci√≥n: USA pero usa kilos
                        else:
                            usar_libras = True  # Otras ciudades USA usan libras
                        destination = dest_name
                        break

            # Tambi√©n detectar patrones de env√≠o espec√≠ficos (solo si ya menciona flete)
            if menciona_flete and not destination:
                # Patrones m√°s espec√≠ficos para detectar destinos (incluyendo CFR/CIF)
                envio_specific_patterns = [
                    r'cfr\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+para|\s+con|\s+de|$)',        # "CFR Lisboa"
                    r'cif\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+para|\s+con|\s+de|$)',        # "CIF Lisboa"
                    r'c&f\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+para|\s+con|\s+de|$)',        # "C&F Lisboa"
                    r'flete\s+a\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+para|\s+con|\s+de|$)',  # "flete a jap√≥n"
                    r'envio\s+a\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+para|\s+con|\s+de|$)',  # "env√≠o a china"
                    r'hacia\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+para|\s+con|\s+de|$)',      # "hacia europa"
                    r'shipping\s+to\s+([a-zA-Z\s]+?)(?:\s+for|\s+with|$)',         # "shipping to japan"
                ]

                for pattern in envio_specific_patterns:
                    match = re.search(pattern, message_lower)
                    if match:
                        dest_detected = match.group(1).strip()

                        # Verificar si coincide con alg√∫n destino conocido
                        for dest_name, patterns in destination_patterns.items():
                            if any(p in dest_detected for p in patterns):
                                destination = dest_name
                                # Configurar usar_libras seg√∫n el destino
                                if dest_name in ['Houston', 'Miami', 'New York', 'Los Angeles', 'Chicago', 'Dallas']:
                                    usar_libras = True if dest_name != 'Houston' else False
                                else:
                                    usar_libras = False  # Pa√≠ses internacionales usan kilos
                                break

                        # Si no coincide con destinos conocidos, usar el texto detectado
                        if not destination:
                            destination = dest_detected.title()
                            usar_libras = False  # Por defecto kilos para destinos desconocidos
                        break
                envio_patterns = [
                    r'flete a ([a-zA-Z\s]+)', r'envio a ([a-zA-Z\s]+)', r'enviar a ([a-zA-Z\s]+)',
                    r'shipping to ([a-zA-Z\s]+)', r'con flete a ([a-zA-Z\s]+)'
                ]

                destination_patterns = {
                    'Houston': ['houston', 'houton', 'huston'],
                    'Miami': ['miami', 'maiami', 'florida'],
                    'New York': ['new york', 'nueva york', 'ny', 'newyork'],
                    'Los Angeles': ['los angeles', 'la', 'california'],
                    'Chicago': ['chicago', 'chicaco'],
                    'Dallas': ['dallas', 'dalas']
                }

                for pattern in envio_patterns:
                    match = re.search(pattern, message_lower)
                    if match:
                        dest_word = match.group(1).lower().strip()
                        # Verificar si es una ciudad USA conocida
                        for dest_name, patterns in destination_patterns.items():
                            if any(p in dest_word for p in patterns):
                                if dest_name == 'Houston':
                                    usar_libras = False  # Houston usa kilos
                                else:
                                    usar_libras = True  # Otras ciudades USA usan libras
                                destination = dest_name
                                break
                        if not destination:
                            destination = dest_word.title()
                        break

            # Detectar nombre del cliente con patrones m√°s amplios (espa√±ol e ingl√©s)
            cliente_nombre = None
            cliente_patterns = [
                # Patrones en espa√±ol
                r'cliente\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+con|\s+de|\s+para|\s+precio|$)',
                r'para\s+(?:el\s+cliente\s+)?([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+con|\s+de|\s+precio|$)',
                r'proforma\s+para\s+(?:el\s+cliente\s+)?([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+con|\s+de|\s+precio|$)',
                r'cotizacion\s+para\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+con|\s+de|\s+precio|$)',
                r'se√±or\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+con|\s+de|\s+precio|$)',
                r'sr\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+con|\s+de|\s+precio|$)',
                # Patrones en ingl√©s
                r'client\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+with|\s+for|\s+price|$)',
                r'for\s+(?:the\s+client\s+)?([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+with|\s+for|\s+price|$)',
                r'proforma\s+for\s+(?:the\s+client\s+)?([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+with|\s+for|\s+price|$)',
                r'quote\s+for\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+with|\s+for|\s+price|$)',
                r'mr\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+with|\s+for|\s+price|$)',
                r'mrs\s+([a-z√°√©√≠√≥√∫√±\w\s]+?)(?:\s+with|\s+for|\s+price|$)'
            ]

            for pattern in cliente_patterns:
                match = re.search(pattern, message_lower)
                if match:
                    cliente_nombre = match.group(1).strip()
                    # Limpiar palabras comunes que no son nombres (espa√±ol e ingl√©s)
                    stop_words = [
                        'el', 'la', 'con', 'de', 'para', 'precio', 'tipo', 'glaseo',
                        'flete', 'producto', 'talla', 'envio', 'destino', 'kilo', 'kilos',
                        'the', 'with', 'for', 'price', 'type', 'glaze', 'freight',
                        'product', 'size', 'shipping', 'destination'
                    ]
                    cliente_words = [word for word in cliente_nombre.split() if word not in stop_words]
                    if cliente_words and len(' '.join(cliente_words)) > 2:
                        cliente_nombre = ' '.join(cliente_words)
                        break

            # Detectar idioma
            english_keywords = ['quote', 'price', 'cost', 'freight', 'shipping', 'quotation', 'shrimp', 'product']
            spanish_keywords = ['proforma', 'cotizacion', 'precio', 'flete', 'envio', 'camaron', 'producto', 'glaseo']

            english_count = sum(1 for keyword in english_keywords if keyword in message_lower)
            spanish_count = sum(1 for keyword in spanish_keywords if keyword in message_lower)

            language = "en" if english_count > spanish_count else "es"

            # Detectar cantidad
            quantity = None
            quantity_patterns = [
                r'(\d+(?:,\d{3})*)\s*(?:libras?|lb|lbs)',
                r'(\d+(?:,\d{3})*)\s*(?:kilos?|kg|kgs)',
                r'(\d+(?:,\d{3})*)\s*(?:toneladas?|tons?)',
                r'(\d+(?:\.\d+)?)\s*(?:mil|thousand)',
                r'(\d+(?:,\d{3})*)\s*(?:pounds?)'
            ]

            for pattern in quantity_patterns:
                match = re.search(pattern, message_lower)
                if match:
                    quantity = match.group(1)
                    break

            # Determinar confianza basada en informaci√≥n extra√≠da
            confidence = 0.6  # Base
            if size: confidence += 0.2
            if product: confidence += 0.1
            if destination: confidence += 0.1
            if glaseo_factor: confidence += 0.1

            return {
                "intent": "proforma",
                "product": product,
                "size": size,
                "quantity": quantity,
                "destination": destination,
                "glaseo_factor": glaseo_factor,
                "glaseo_percentage": glaseo_percentage_original,  # Porcentaje original solicitado
                "flete_custom": flete_custom,  # Valor de flete personalizado detectado
                "is_ddp": menciona_ddp,  # Flag para indicar que es precio DDP (ya incluye flete)
                "usar_libras": usar_libras,
                "cliente_nombre": cliente_nombre,
                "wants_proforma": True,
                "language": language,  # Idioma detectado
                "confidence": min(confidence, 0.95),  # M√°ximo 0.95
                "suggested_response": "Procesar proforma con datos extra√≠dos"
            }

        # Patrones de productos
        product_patterns = ['producto', 'productos', 'camaron', 'camarones', 'hlso', 'hoso', 'p&d']
        if any(pattern in message_lower for pattern in product_patterns):
            return {
                "intent": "product_info",
                "product": None,
                "size": None,
                "quantity": None,
                "destination": None,
                "confidence": 0.7,
                "suggested_response": "Mostrar informaci√≥n de productos"
            }

        # Patrones de ayuda
        help_patterns = ['ayuda', 'help', 'como', 'que puedes', 'opciones', '?']
        if any(pattern in message_lower for pattern in help_patterns):
            return {
                "intent": "help",
                "product": None,
                "size": None,
                "quantity": None,
                "destination": None,
                "confidence": 0.8,
                "suggested_response": "Mostrar men√∫ de ayuda"
            }

        return {
            "intent": "unknown",
            "product": None,
            "size": None,
            "quantity": None,
            "destination": None,
            "confidence": 0.3,
            "suggested_response": "Mostrar men√∫ principal"
        }

    def get_smart_fallback_response(self, user_message: str, intent_data: dict) -> str | None:
        """
        Genera respuestas inteligentes sin IA basadas en patrones
        """
        intent = intent_data.get('intent', 'unknown')
        message_lower = user_message.lower().strip()

        if intent == 'greeting':
            # Respuesta r√°pida y directa para saludos
            return "¬°Hola! ü¶ê ¬øQu√© producto de camar√≥n necesitas? Te genero la cotizaci√≥n al instante üí∞"

        elif intent == 'pricing':
            return "üí∞ ¬°Perfecto! ¬øQu√© producto necesitas? HLSO es muy popular. Escribe 'precios' para ver tallas y crear tu proforma üìã"

        elif intent == 'product_info':
            return "ü¶ê Tenemos HLSO, P&D IQF, HOSO y m√°s. ¬øCu√°l te interesa? Te genero la cotizaci√≥n con precios FOB actualizados üí∞"

        elif intent == 'help':
            return "ü§ñ Te ayudo a crear proformas de camar√≥n:\n‚Ä¢ Precios FOB actualizados\n‚Ä¢ Todas las tallas disponibles\n‚Ä¢ PDF profesional\n\n¬øQu√© producto necesitas? ü¶ê"

        else:
            return "ü¶ê ¬°Hola! Soy ShrimpBot de BGR Export. ¬øQu√© camar√≥n necesitas? Te genero la proforma al instante üìãüí∞"

    def _clean_problematic_emojis(self, text: str) -> str:
        """
        Limpia emojis que pueden causar problemas de codificaci√≥n en WhatsApp
        """
        # Lista de emojis problem√°ticos y sus reemplazos
        problematic_emojis = {
            'ü§ë': 'üí∞',  # Reemplazar cara con dinero por bolsa de dinero
            'ü§ñ': 'ü¶ê',  # Reemplazar robot por camar√≥n
            'üí∏': 'üí∞',  # Reemplazar dinero volando por bolsa de dinero
            'ü§î': 'ü§ù',  # Reemplazar cara pensando por apret√≥n de manos
        }

        cleaned_text = text
        for problematic, replacement in problematic_emojis.items():
            cleaned_text = cleaned_text.replace(problematic, replacement)

        return cleaned_text

    def _make_request_with_retry(self, messages: list[dict], max_tokens: int = 300, temperature: float = 0.3, max_retries: int = 3) -> str | None:
        """
        Hace petici√≥n a OpenAI con reintentos autom√°ticos
        """
        for attempt in range(max_retries):
            try:
                result = self._make_request(messages, max_tokens, temperature)
                if result:
                    return result

                # Si falla, esperar un poco antes de reintentar
                if attempt < max_retries - 1:
                    import time
                    time.sleep(1 * (attempt + 1))  # Backoff exponencial

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Intento {attempt + 1}/{max_retries} fall√≥: {str(e)}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(1 * (attempt + 1))

        return None

    def _intelligent_fallback(self, user_message: str, session_data: dict = None) -> dict:
        """
        Fallback inteligente que SIEMPRE responde algo coherente
        Maneja cualquier tipo de solicitud del usuario
        """
        message_lower = user_message.lower().strip()

        # Detectar saludos
        greeting_patterns = [r'\bhola\b', r'\bhello\b', r'\bhi\b', r'\bbuenos\b', r'\bbuenas\b',
                           r'\bhey\b', r'\bqu√© tal\b', r'\bc√≥mo est√°s\b']
        if any(re.search(pattern, message_lower) for pattern in greeting_patterns):
            return {
                "response": "¬°Hola! üëã Soy ShrimpBot de BGR Export ü¶ê\n\n¬øQu√© producto necesitas? Te ayudo a crear tu cotizaci√≥n al instante üí∞",
                "action": "greeting",
                "data": {}
            }

        # Detectar solicitudes de audio/voz
        audio_patterns = ['audio', 'voz', 'voice', 'nota de voz', 'mensaje de voz', 'grabaci√≥n']
        if any(pattern in message_lower for pattern in audio_patterns):
            return {
                "response": "üé§ ¬°Claro! Puedes enviarme notas de voz y las procesar√© autom√°ticamente.\n\nSolo env√≠a tu audio y te responder√© con la informaci√≥n que necesites ü¶ê",
                "action": "audio_info",
                "data": {}
            }

        # Detectar preguntas sobre productos
        product_patterns = ['producto', 'productos', 'qu√© tienen', 'qu√© venden', 'cat√°logo', 'opciones']
        if any(pattern in message_lower for pattern in product_patterns):
            return {
                "response": "ü¶ê Productos disponibles:\n\n‚Ä¢ HLSO (sin cabeza)\n‚Ä¢ HOSO (con cabeza)\n‚Ä¢ P&D IQF (pelado)\n‚Ä¢ P&D BLOQUE\n‚Ä¢ EZ PEEL\n‚Ä¢ PuD-EUROPA\n‚Ä¢ COOKED\n\n¬øCu√°l te interesa? üí∞",
                "action": "product_list",
                "data": {}
            }

        # Detectar preguntas sobre precios
        price_patterns = ['precio', 'precios', 'cu√°nto', 'cuanto', 'costo', 'cost', 'value']
        if any(pattern in message_lower for pattern in price_patterns):
            return {
                "response": "üí∞ Te genero cotizaciones con precios FOB actualizados.\n\n¬øQu√© producto y talla necesitas?\nEjemplo: HLSO 16/20 ü¶ê",
                "action": "price_inquiry",
                "data": {}
            }

        # Detectar solicitudes de ayuda
        help_patterns = ['ayuda', 'help', 'c√≥mo', 'como funciona', 'qu√© puedes', 'opciones']
        if any(pattern in message_lower for pattern in help_patterns):
            return {
                "response": "ü§ñ Te ayudo a crear proformas de camar√≥n:\n\n‚úÖ Precios FOB actualizados\n‚úÖ Todas las tallas\n‚úÖ PDF profesional\n‚úÖ C√°lculo de glaseo\n‚úÖ Flete incluido\n\n¬øQu√© producto necesitas? ü¶ê",
                "action": "help",
                "data": {}
            }

        # Detectar agradecimientos
        thanks_patterns = ['gracias', 'thanks', 'thank you', 'muchas gracias', 'te agradezco']
        if any(pattern in message_lower for pattern in thanks_patterns):
            return {
                "response": "¬°De nada! üòä Estoy aqu√≠ para ayudarte.\n\n¬øNecesitas algo m√°s? ü¶ê",
                "action": "thanks",
                "data": {}
            }

        # Detectar despedidas
        goodbye_patterns = ['adi√≥s', 'adios', 'bye', 'chao', 'hasta luego', 'nos vemos']
        if any(pattern in message_lower for pattern in goodbye_patterns):
            return {
                "response": "¬°Hasta pronto! üëã Cuando necesites cotizaciones, aqu√≠ estar√© ü¶êüí∞",
                "action": "goodbye",
                "data": {}
            }

        # Detectar tallas espec√≠ficas (intento de cotizaci√≥n)
        if re.search(r'\d+/\d+', message_lower):
            return {
                "response": "üìä Detect√© una talla en tu mensaje.\n\n¬øQu√© producto necesitas?\n‚Ä¢ HLSO (sin cabeza)\n‚Ä¢ HOSO (con cabeza)\n‚Ä¢ P&D IQF (pelado)\n\nEscribe el producto para generar tu cotizaci√≥n ü¶ê",
                "action": "size_detected",
                "data": {}
            }

        # Si tiene contexto de sesi√≥n, usar eso
        if session_data and session_data.get('products'):
            products = session_data['products']
            products_str = ", ".join([f"{p['product']} {p['size']}" for p in products])
            return {
                "response": f"üìã Tienes en tu sesi√≥n: {products_str}\n\n¬øQuieres generar la proforma o modificar algo? ü¶ê",
                "action": "session_context",
                "data": session_data
            }

        # Respuesta gen√©rica pero √∫til para CUALQUIER otra cosa
        return {
            "response": "ü¶ê Soy ShrimpBot de BGR Export.\n\nTe ayudo a crear cotizaciones de camar√≥n premium.\n\n¬øQu√© producto necesitas?\nEjemplo: HLSO 16/20 üí∞",
            "action": "general_inquiry",
            "data": {}
        }

    def process_audio_message(self, audio_file_path: str, session_data: dict = None, conversation_history: list[dict] = None) -> dict:
        """
        Procesa un mensaje de audio completo: transcribe y responde
        
        Args:
            audio_file_path: Ruta al archivo de audio
            session_data: Datos de la sesi√≥n actual
            conversation_history: Historial de conversaci√≥n
        
        Returns:
            Dict con transcripci√≥n, respuesta y acciones
        """
        try:
            # Transcribir audio
            transcription = self.transcribe_audio(audio_file_path)

            if not transcription:
                return {
                    "response": "üé§ No pude entender el audio. ¬øPodr√≠as enviarlo de nuevo o escribir tu mensaje? ü¶ê",
                    "action": "audio_transcription_failed",
                    "data": {},
                    "transcription": None
                }

            logger.info(f"‚úÖ Audio transcrito: '{transcription}'")

            # Procesar el mensaje transcrito
            result = self.handle_any_request(transcription, session_data, conversation_history)

            # Agregar transcripci√≥n al resultado
            result['transcription'] = transcription
            result['input_type'] = 'audio'

            return result

        except Exception as e:
            logger.error(f"‚ùå Error procesando audio: {str(e)}")
            return {
                "response": "üé§ Hubo un problema con el audio. Por favor intenta de nuevo o escribe tu mensaje ü¶ê",
                "action": "audio_processing_error",
                "data": {},
                "transcription": None
            }

    def handle_any_request(self, user_message: str, session_data: dict = None, conversation_history: list[dict] = None) -> dict:
        """
        M√âTODO PRINCIPAL: Maneja CUALQUIER tipo de solicitud del usuario
        Garantiza que siempre haya una respuesta coherente
        
        Args:
            user_message: Mensaje del usuario (texto o transcripci√≥n de audio)
            session_data: Datos de la sesi√≥n actual
            conversation_history: Historial de conversaci√≥n
        
        Returns:
            Dict con respuesta garantizada y acciones
        """
        try:
            # Validar entrada
            if not user_message or not user_message.strip():
                return {
                    "response": "ü§î No recib√≠ ning√∫n mensaje. ¬øPodr√≠as escribir o enviar audio de nuevo? ü¶ê",
                    "action": "empty_message",
                    "data": {}
                }

            # Limpiar mensaje
            user_message = user_message.strip()

            # Intentar con IA primero
            if self.is_available():
                try:
                    result = self.chat_with_context(user_message, conversation_history, session_data)
                    if result and result.get('response'):
                        return result
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error en IA, usando fallback: {str(e)}")

            # Si falla IA o no est√° disponible, usar fallback inteligente
            return self._intelligent_fallback(user_message, session_data)

        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico en handle_any_request: {str(e)}")
            # √öltima l√≠nea de defensa: respuesta de emergencia
            return {
                "response": "ü¶ê Soy ShrimpBot de BGR Export.\n\n¬øQu√© producto de camar√≥n necesitas? Te ayudo a crear tu cotizaci√≥n üí∞",
                "action": "emergency_fallback",
                "data": {}
            }
